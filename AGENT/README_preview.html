<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>README Preview</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
            max-width: 900px;
            margin: 40px auto;
            padding: 20px;
            line-height: 1.7;
            color: #333;
        }
        h1 {
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }
        h2 {
            margin-top: 30px;
            border-bottom: 1px solid #eee;
            padding-bottom: 5px;
        }
        h3 {
            margin-top: 20px;
            color: #555;
        }
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Monaco', 'Courier New', monospace;
        }
        pre {
            background: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            border-left: 4px solid #007acc;
        }
        ul, ol {
            margin: 10px 0;
        }
        li {
            margin: 5px 0;
        }
        strong {
            color: #000;
        }
    </style>
</head>
<body>
<h2>AutoStream Conversational AI Agent (Gemini)</h2>
<p>This project demonstrates a stateful conversational AI agent that converts conversations into qualified leads for a fictional SaaS product, AutoStream.</p>
<h3>Features</h3>
<ul>
<li><strong>Intent detection</strong> using Gemini 2.0 Flash</li>
<li><strong>RAG with local JSON knowledge base</strong></li>
<li><strong>Stateful multi-turn conversation</strong></li>
<li><strong>Mock lead capture tool</strong> (saves leads to <code>data/leads.json</code>)</li>
<li><strong>Streamlit-based UI</strong></li>
</ul>
<h3>How to Run Locally</h3>
<p><strong>1. Clone or navigate to the project directory:</strong></p>
<pre><code class="language-bash">cd autostream-agent-gemini
</code></pre>
<p><strong>2. Create a virtual environment (recommended):</strong></p>
<pre><code class="language-bash">python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
</code></pre>
<p><strong>3. Install dependencies:</strong></p>
<pre><code class="language-bash">pip install -r requirements.txt
</code></pre>
<p><strong>4. Set up your Google API key:</strong></p>
<p>Create a <code>.env</code> file in the project root:</p>
<pre><code class="language-text">GOOGLE_API_KEY=YOUR_API_KEY_HERE
</code></pre>
<p>You can get your API key from <a href="https://makersuite.google.com/app/apikey">Google AI Studio</a>.</p>
<p><strong>5. Run the application:</strong></p>
<p><strong>Option A: Streamlit UI (Recommended)</strong></p>
<pre><code class="language-bash">streamlit run ui.py
</code></pre>
<p>Then open your browser to <code>http://localhost:8501</code></p>
<p><strong>Option B: CLI Mode</strong></p>
<pre><code class="language-bash">python main.py
</code></pre>
<p><strong>Note:</strong> The first run may take a moment to initialize. Make sure you have an active internet connection for API calls.</p>
<h3>Architecture Explanation</h3>
<p>This project uses <strong>LangChain</strong> (not LangGraph or AutoGen) with Gemini 2.0 Flash to implement a stateful, task-focused conversational agent for AutoStream. The agent logic is split into focused modules: <code>intent.py</code> for intent classification using Gemini's LLM, <code>rag.py</code> for knowledge retrieval from a local JSON knowledge base, <code>tools.py</code> for lead capture execution, and <code>graph.py</code> as a controller that routes messages based on intent and state.</p>
<p><strong>Why you chose LangChain (not LangGraph/AutoGen):</strong></p>
<p>LangChain was chosen over LangGraph/AutoGen for three key reasons. First, <strong>simplicity</strong>: LangChain provides clean abstractions around prompts, LLM calls, and tools without forcing a heavy orchestration layer, making it ideal for straightforward conversational flows. Second, <strong>flexibility</strong>: The modular design allows easy customization of intent detection, RAG retrieval, and tool execution without being locked into a rigid graph structure. Third, <strong>lightweight</strong>: For this use case, a simple state machine is sufficient—we don't need the complex multi-agent orchestration that LangGraph/AutoGen provide. This modular approach makes the codebase maintainable and allows each component to be tested and modified independently.</p>
<p><strong>How state is managed:</strong></p>
<p>State is managed explicitly using a lightweight <code>AgentState</code> dataclass that stores the current intent, lead details (name, email, platform), conversation history, and a flag indicating whether the lead has been captured. In the Streamlit UI, this state object persists in <code>st.session_state</code>, surviving across multiple turns and supporting multi-step flows like collecting lead information over 5–6 messages. In CLI mode, the state object lives in memory for the session duration. The RAG component uses keyword-based retrieval from a local JSON knowledge base, ensuring answers stay grounded in AutoStream's pricing plans and policies. When high intent is detected and all lead fields are collected, the agent calls <code>mock_lead_capture</code> which saves leads to <code>data/leads.json</code> for tracking and future reference.</p>
<h3>WhatsApp Deployment (Webhook Integration)</h3>
<p>To integrate this agent with WhatsApp using webhooks, deploy a webhook server (FastAPI, Flask, or similar) over HTTPS and configure the WhatsApp Business Cloud API to send incoming messages to your endpoint. When a user sends a message, WhatsApp sends a POST request containing the user's phone number, message text, and metadata.</p>
<p>For each incoming message, extract the WhatsApp user ID and look up or initialize an <code>AgentState</code> object for that user, stored in a database (Redis or PostgreSQL) keyed by WhatsApp user ID. This ensures each user has their own conversation state that persists across sessions. Pass the message text and retrieved state to the existing <code>agent_step()</code> function—the agent handles intent detection, RAG retrieval, and lead capture as usual. After processing, update the stored state to persist conversation history and lead capture progress. To reply, call the WhatsApp Cloud API "send message" endpoint with the agent's response.</p>
<p>The key benefit is that the existing agent code requires zero changes. WhatsApp provides the transport layer, the webhook server manages state persistence, and the agent handles all reasoning. Implement webhook signature validation, error handling with retries, and rate limiting for security and reliability.</p>
<h3>LLM Choice</h3>
<p>Gemini 2.0 Flash is used for its low latency and availability of a free tier.</p>
</body>
</html>